{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Annotation Sampling for Manual Quality Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes the methods used for basic data clean up for ease of downstream analysis and the random sampling of relationship annotations for manual quality inspection. Mark2Cure is an open source, citizen science effort. Although this particular notebook could be a lot shorter, it is intended to be easy to follow along for people WITHOUT bioinformatics and programming experience and will hopefully serve as an easy-to-follow introduction to some basic [python](https://en.wikibooks.org/wiki/Python_Programming) and [pandas](https://pandas.pydata.org/) functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import [modules](https://en.wikibooks.org/wiki/Python_Programming/Modules) and [dictionaries](https://en.wikibooks.org/wiki/Python_Programming/Dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import relationship_dictionaries\n",
    "from m2c_rel_basic import add_type_cols\n",
    "from m2c_rel_basic import split_out_testers\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual relationship annotations made by users are saved as hash codes. To make sense of it, import the dictionaries available for translating them. The dictionaries can be found in the relationship_dictionaries.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rel_hash_dict,redundant_response_dict,abbreviated_rels_dict,abbreviated_rels_dict_4_hash,concept_broken_dict,concept_not_broken_dict = relationship_dictionaries.load_RE_dictionaries()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Format, and Clean up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the exported relationship annotations data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28059\n"
     ]
    }
   ],
   "source": [
    "datasource = '2017.11.22 RE anns export.txt'\n",
    "savepath = 'data/'\n",
    "exppath = 'exports/'\n",
    "filesrc = savepath+datasource\n",
    "all_data_imported = read_csv(filesrc, delimiter='\\t', header=0)\n",
    "#print(all_data_imported.head(2))\n",
    "print(len(all_data_imported))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For concept 1 and concept 2, add the concept type based on the information in the relation_type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data_imported.rename(columns={'relation_type':'reltype','answer':'evtype','concept_1_id':'refid1','concept_2_id':'refid2'}, inplace=True)\n",
    "all_data_imported['concept_pair']=all_data_imported['refid1'].astype(str).str.cat(all_data_imported['refid2'].astype(str),sep=\"_x_\")\n",
    "all_relation_anns = add_type_cols(all_data_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the user responses using the dictionaries in order to make the data more interpretable. The rel_hash_dict translates the relationship annotation hash code to the response that the user selected. Note that some options are redundant and were included based on internal usability study results indicating that users were more comfortable having such options. The redundant_response_dict merges redundant responses so that they are treated appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notice that: \n",
      "\n",
      "   user_id                                    evtype reltype     pmid\n",
      "0      364  52d80rv4t0h0g14gb83oamjfm8h9rz19zl1ubzku     g_d  9621534\n",
      "1      364  zl4RlTGwZM9Ud3CCXpU2VZa7eQVnJj0MdbsRBMGy     g_d  9621534\n",
      "\n",
      " becomes: \n",
      "\n",
      "   user_id                           evtype reltype     pmid\n",
      "0      364  gene has no relation to disease     g_d  9621534\n",
      "1      364                       c_1_broken     g_d  9621534\n"
     ]
    }
   ],
   "source": [
    "print('notice that: \\n')\n",
    "print(all_relation_anns[['user_id','evtype','reltype','pmid']].head(n=2))\n",
    "all_relation_anns.replace({'evtype':rel_hash_dict}, inplace=True)\n",
    "all_relation_anns.replace({'evtype':redundant_response_dict}, inplace=True)\n",
    "print('\\n becomes: \\n')\n",
    "print(all_relation_anns[['user_id','evtype','reltype','pmid']].head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A single task unit is the annotation of a specific concept pair from a specific abstract (pmid). Hence, a single task can be identified by the concept pair and pmid.**\n",
    "\n",
    "To get a count of the number of users that classified each task, we use the pandas groupby and size functions.  The groupby function will group the data in the table by the values of whatever columns you specify. The size function produces the number of unique rows in the table.\n",
    "\n",
    "Group the relationship table by pmid and concept and obtain the number of rows (ie- size) that each task appeared in. This will result in a table with unique tasks and a count of the number of times each task was done by the Mark2Cure community. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique tasks done by users:  4047 \n",
      "\n",
      "total number of unique tasks considered complete (done by 15 users):  1009\n"
     ]
    }
   ],
   "source": [
    "## Group relationship table by pmid and concept pair to get number of users that classified each concept pair\n",
    "rel_ann_counts = all_relation_anns.groupby(['pmid','refid1','refid2','reltype','concept_pair']).size().reset_index(name='user_count')\n",
    "pmid_counts = all_relation_anns.groupby(['pmid']).size().reset_index(name='pmid_count')\n",
    "print('total number of unique tasks done by users: ',len(rel_ann_counts),'\\n')\n",
    "\n",
    "## Pull annotations that have been completed by at least 15 users\n",
    "threshold = 15\n",
    "ann_threshold = rel_ann_counts.loc[rel_ann_counts['user_count']>=threshold]\n",
    "completed_conceptpairs = rel_ann_counts.loc[rel_ann_counts['user_count']>=15]\n",
    "print('total number of unique tasks considered complete (done by 15 users): ', len(ann_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark2Cure does not have a separate development server; thus, internal testing is sometimes performed on the live production server using test accounts. The data from these accounts should always be excluded from downstream analysis. \n",
    "\n",
    "For our downstream analysis we want to focus on annotations tasks that can be considered complete. These are tasks which have been done by at least 15 users and should no longer be available for users to work on. Because test accounts may have done some of these tasks, we distinguish between true annotations (submitted by users) and test annotations (test submissions).\n",
    "\n",
    "Going back to our relationship annotation table (with concept types added), we can use the split_out_testers function to split this relationship annotation table into separate tables: one without test annotations (filtered results) and one with only test annotations (test_anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split the relationship annotation table into true responses vs test responses\n",
    "filtered_results, test_anns, test_account_list = split_out_testers(all_relation_anns)\n",
    "#print(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up the nontest, user annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the number of times each response (evtype) was selected for each unique task (pmid,concept pair)\n",
    "nontest_anns = filtered_results.copy()\n",
    "cprelation_counts = nontest_anns.groupby(['pmid','concept_pair','reltype','evtype','refid1','refid2']).size().reset_index(name='relation_count')\n",
    "#print(cprelation_counts.head(n=5))\n",
    "\n",
    "## Get the number of users that annotated each task (eg- number of times the task was done by real users)\n",
    "nontest_counts = nontest_anns.groupby(['pmid','concept_pair']).size().reset_index(name='true_completions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the test annotations for inclusion in the cleaned up data file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pmid    concept_pair reltype                   evtype refid1   refid2  \\\n",
      "0  1325164   5443_x_202200     g_d  gene relates to disease   5443   202200   \n",
      "1  1325164  5443_x_C536009     g_d  gene relates to disease   5443  C536009   \n",
      "2  1325164  5443_x_D004931     g_d  gene relates to disease   5443  D004931   \n",
      "3  1325164  5443_x_D035583     g_d               c_2_broken   5443  D035583   \n",
      "4  1325164  5443_x_D052439     g_d               c_2_broken   5443  D052439   \n",
      "\n",
      "   test_completions  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 1  \n",
      "3                 1  \n",
      "4                 1  \n"
     ]
    }
   ],
   "source": [
    "## Get the test completion counts\n",
    "test_counts = test_anns.groupby(['pmid','concept_pair','reltype','evtype','refid1','refid2']).size().reset_index(name='test_completions')\n",
    "print(test_counts.head(n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to use pandas for filtering data. One way is to use the merge function which essentially merges the data from multiple tables into one based on selected columns they have in common. There is a nice introductory explanation of pandas merge types [here](https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/#mergetypes). By using a left merge, we can very quickly pull the corresponding data from an unfiltered table into a filtered one, resulting in an expanded filtered table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pmid   refid1   refid2 reltype       concept_pair  user_count  \\\n",
      "0  1299347  C095810  D008232     c_d  C095810_x_D008232          15   \n",
      "1  1299347  C095810  D008232     c_d  C095810_x_D008232          15   \n",
      "2  1299347  D005944  D008232     c_d  D005944_x_D008232          15   \n",
      "3  1299347  D005944  D008232     c_d  D005944_x_D008232          15   \n",
      "4  1299347  D005944  D008232     c_d  D005944_x_D008232          15   \n",
      "\n",
      "                                   evtype  relation_count  test_completions  \\\n",
      "0                              c_1_broken              14               0.0   \n",
      "1             drug (may) cause(s) disease               1               0.0   \n",
      "2                              c_1_broken               8               0.0   \n",
      "3  drug (may) increase(s) risk of disease               1               0.0   \n",
      "4         drug has no relation to disease               3               0.0   \n",
      "\n",
      "   true_responses  response_ratio  \n",
      "0              15        0.933333  \n",
      "1              15        0.066667  \n",
      "2              15        0.533333  \n",
      "3              15        0.066667  \n",
      "4              15        0.200000  \n"
     ]
    }
   ],
   "source": [
    "## Add the data on test completions as a column to the non-test table for ease of downstream analysis\n",
    "tmpjoined = cprelation_counts.merge(test_counts,on=['pmid','concept_pair','reltype','evtype','refid1','refid2'],how='left')\n",
    "\n",
    "## Use completed_conceptpairs table to select only cp/pmids in cprelation_counts table that have at been completed by at least 15 users\n",
    "annresults = pandas.merge(completed_conceptpairs, tmpjoined, on=['pmid', 'concept_pair','reltype','refid1','refid2'], how='left').fillna(0)\n",
    "\n",
    "## Calculate number of true completions by subtracting out the test completions\n",
    "annresults['true_responses'] = annresults['user_count'].astype(int).sub(annresults['test_completions'].astype(int))\n",
    "\n",
    "## obtain ratios for each answer selected for each task\n",
    "annresults['response_ratio'] = annresults['relation_count'].astype(int).div(annresults['true_responses'].astype(int))\n",
    "print(annresults.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the annotations completed by at least 15 users for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task submissions in the set of completed Relationship tasks:  15739\n",
      "Unique pmid-specific concept pairs (ie-relation tasks) completed:  1009\n"
     ]
    }
   ],
   "source": [
    "## Add the true response and response_ratio data for completed annotations to the original nontest annotation table.\n",
    "all_anns = nontest_anns.merge(annresults,on=('concept_pair','refid1','refid2','reltype','pmid','evtype'),how='left').fillna(-1)\n",
    "all_completed_anns = all_anns.loc[all_anns['true_responses']!=-1].copy()\n",
    "print('Task submissions in the set of completed Relationship tasks: ',len(all_completed_anns))\n",
    "\n",
    "## Create the set of all concept pairs x pmid done by real users\n",
    "all_cp_pmids = all_anns.groupby(['pmid','concept_pair','refid1','refid2','refid1_type','refid2_type','reltype']).size().reset_index(name='counts')\n",
    "\n",
    "## Create a unique task identifier by hashing the concept pair and pmids for ease of downstream analysis\n",
    "all_completed_anns['cpmid'] = all_completed_anns['pmid'].astype(str).str.cat(all_completed_anns['concept_pair'].astype(str), sep='_')\n",
    "all_cp_pmids['cpmid'] = all_cp_pmids['pmid'].astype(str).str.cat(all_cp_pmids['concept_pair'].astype(str), sep='_')\n",
    "\n",
    "cpmid_set = all_completed_anns['cpmid'].unique().tolist()\n",
    "print('Unique pmid-specific concept pairs (ie-relation tasks) completed: ',len(cpmid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned up and completed annotation data for future downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Export the completed annotations dataframe for future analysis\n",
    "annresults.to_csv(savepath+'annresults.txt', sep='\\t', header=True)\n",
    "all_completed_anns.to_csv(savepath+'all_completed_anns.txt', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_completed_anns_pmids = all_completed_anns[['pmid']].drop_duplicates(keep='first').reset_index(drop=True)\n",
    "all_completed_anns_pmids.to_csv(exppath+'all_completed_anns_pmids.txt', sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a 10% sample of the completed annotations for manual quality inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of the cp_pmids. \n",
    "The set of unique tasks (cpmids) (with test accounts removed) is 1009 cpmids in length.  \n",
    "10% of this would be about 100 cpmids.  \n",
    "Do 4 samples of 30 unique cpmids which may allow for expert inconsistency as well.  \n",
    "Export the samples for manual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interations_to_try=4\n",
    "samples_per_iteration = 30\n",
    "i=0\n",
    "analysis_results = []\n",
    "sampling_table = pandas.DataFrame(columns=['pmid','concept_pair','refid1','refid2','refid1_type','refid2_type','reltype','cpmid'])\n",
    "\n",
    "while i<interations_to_try:\n",
    "    sampling_set = random.sample(cpmid_set, samples_per_iteration)\n",
    "    for each_cpmid in sampling_set:\n",
    "        tmp_chk = all_cp_pmids.loc[all_cp_pmids['cpmid']==each_cpmid]\n",
    "        sampling_table = pandas.concat((sampling_table,tmp_chk))\n",
    "    sampling_table.to_csv(exppath+'sample_'+str(i)+'_for_expert_ann.txt', sep='\\t', header=True) \n",
    "    i=i+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
