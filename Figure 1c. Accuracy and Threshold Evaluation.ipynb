{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Relationship Annotation Accuracy Based on a QC'd Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the evaluation of user annotations in comparison of the randomly sampled set of completed tasks that have been manually annotated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import m2c_rel_basic\n",
    "import relationship_dictionaries\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as mplot\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relationship annotations data for only completed concept pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savepath = 'data/'\n",
    "exppath = 'results/'\n",
    "all_completed_anns = read_csv(exppath+'all_completed_anns.txt', delimiter='\\t', header=0)\n",
    "all_completed_anns.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Quality Curated Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esample_df = m2c_rel_basic.get_QC_data(savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dictionaries for translating hashed responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_hash_dict,redundant_response_dict,abbreviated_rels_dict,abbreviated_rels_dict_4_hash,concept_broken_dict,concept_not_broken_dict = relationship_dictionaries.load_RE_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the user annotations relative to the quality curated annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pull the annotations marked 'broken' during QC'ing into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expert_broken = esample_df.loc[esample_df['conclusion']=='concept_broken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pull the cpmids (concept pair pmid hashed identifiers) that were QC'd into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expert_cpmids = set(esample_df['cpmid'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the cpmids from the set of annotations that were QC'd and use the cpmids from the QC'd set to pull all user annotations for the same cpmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_cpmids = set(expert_cpmids)\n",
    "total_ref_set = all_completed_anns.loc[all_completed_anns['cpmid'].isin(test_set_cpmids)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviate the user responses for ease of viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_ref_set['evtype'].replace(abbreviated_rels_dict_4_hash, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove distinction between C1 broken and C2 broken since it's technically possible for both to be true AND because it's not important to distinguish it in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_ref_set['evtype'].replace(concept_broken_dict, inplace=True)\n",
    "esample_df['conclusion'].replace(concept_broken_dict, inplace=True)\n",
    "print(total_ref_set.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly subsample the user annotations to determine how worker number (n) relates to agreement with the quality-checked results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the responses and put the results into a dataframe, 10 iterations per n per cpmid. \n",
    "Note that this is the slowest part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n=1\n",
    "majority_response=[]\n",
    "\n",
    "i=0\n",
    "while i<10:\n",
    "    n=1\n",
    "    while n<16:\n",
    "        for each_cpmid in test_set_cpmids:\n",
    "            result_dict = {}\n",
    "            result_dict['iteration'] = i\n",
    "            result_dict['cpmid'] = each_cpmid\n",
    "            result_dict['n'] = n\n",
    "            tmp_df = total_ref_set.loc[total_ref_set['cpmid']==each_cpmid]\n",
    "            user_set = set(tmp_df['user_id'].tolist())\n",
    "            if n<=len(user_set):\n",
    "                user_sample = random.sample(user_set, n)\n",
    "                ref_sample = tmp_df.loc[tmp_df['user_id'].isin(user_sample)]\n",
    "                if n == 1:\n",
    "                    result_dict['response'] = ref_sample.iloc[0]['evtype']\n",
    "                    result_dict['majority?'] = 'yes'\n",
    "                    result_dict['tie?'] = 'no'\n",
    "                    result_dict['result_selection'] = 'single_vote'\n",
    "                else:\n",
    "                    ## Groupby response and get biggest and next biggest value    \n",
    "                    ref_sample_size = ref_sample.groupby(['cpmid','evtype']).size().reset_index(name='counts')\n",
    "                    ref_sample_size.sort_values('counts', ascending=False, inplace=True)\n",
    "                    first_most = ref_sample_size.iloc[0]['counts']\n",
    "                    try:\n",
    "                        sec_most = ref_sample_size.iloc[1]['counts'] ##if this works, results are not unanimous\n",
    "                        if first_most > sec_most: ## If there is a majority\n",
    "                            result_dict['majority?'] = 'simple_majority'\n",
    "                            result_dict['result_selection'] = 'majority'\n",
    "                            result_dict['response'] = ref_sample_size.iloc[0]['evtype']\n",
    "                            result_dict['tie?'] = 'no'\n",
    "                        else: ## else it's a tie\n",
    "                            result_dict['majority?'] = 'no_majority'\n",
    "                            result_dict['result_selection'] = 'random'\n",
    "                            if len(ref_sample_size)==2: ## check if full tie\n",
    "                                rand_option = random.randint(0, 1)\n",
    "                                result_dict['response'] = ref_sample_size.iloc[rand_option]['evtype']\n",
    "                                result_dict['tie?'] = 'two_way'\n",
    "                            elif (len(ref_sample_size) > 2) & (sec_most > ref_sample_size.iloc[3]['counts']): #check if majority tied\n",
    "                                rand_option = random.randint(0, 1)\n",
    "                                result_dict['response'] = ref_sample_size.iloc[rand_option]['evtype']\n",
    "                                result_dict['tie?'] = 'top_two_way'\n",
    "                            elif (len(ref_sample_size) > 2) & (sec_most == ref_sample_size.iloc[3]['counts']): #check for 3-way tie                      \n",
    "                                rand_option = random.randint(0, 2)\n",
    "                                result_dict['response'] = ref_sample_size.iloc[rand_option]['evtype']\n",
    "                                result_dict['tie?'] = '3_way'\n",
    "                            else:    \n",
    "                                rand_option = random.randint(0, len(ref_sample_size))\n",
    "                                result_dict['response'] = ref_sample_size.iloc[rand_option]['evtype']\n",
    "                                result_dict['tie?'] = 'other'\n",
    "                    except: ## Scenario: results are unanimous\n",
    "                        result_dict['majority?'] = 'unanimous'\n",
    "                        result_dict['response'] = ref_sample_size.iloc[0]['evtype']\n",
    "                        result_dict['tie?'] = 'no'\n",
    "                        result_dict['result_selection'] = 'majority'\n",
    "            else:\n",
    "                result_dict['majority?'] = 'n/a'\n",
    "                result_dict['response'] = 'not_enough'\n",
    "                result_dict['tie?'] = 'n/a'\n",
    "                result_dict['result_selection'] = 'n/a' \n",
    "            ## check how well user response matched expert response\n",
    "            tmp_edf = esample_df.loc[esample_df['cpmid']==each_cpmid]           \n",
    "            test_response = result_dict.get('response')\n",
    "            expert_response = tmp_edf['conclusion'].iloc[0]\n",
    "            if test_response=='not_enough':\n",
    "                result_dict['expert_match?'] = 'n/a'\n",
    "            elif test_response == expert_response:\n",
    "                result_dict['expert_match?'] = 'yes'\n",
    "            else:\n",
    "                result_dict['expert_match?'] = 'no'\n",
    "            majority_response.append(result_dict) \n",
    "        n = n+1\n",
    "    i=i+1\n",
    "   \n",
    "\n",
    "majority_df = pandas.DataFrame(majority_response)\n",
    "\n",
    "print(majority_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#majority_df.to_csv(exppath+'raw_random_accuracy_results.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#majority_df = read_csv(exppath+'raw_random_accuracy_results.txt',delimiter='\\t',header=0)\n",
    "#majority_df.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the resulting data, grouping by iteration and threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of expert_match (true positive) vs total number of responses (true positive plus incorrect response) \n",
    "This gives total number of answers that matched vs didn't match the QC'd result for the whole QC cpmid set at every level of n, for each repetition and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_matrix = majority_df.groupby(['iteration','n','expert_match?']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the n/a responses (this is usually when n exceeds sample due to removal of tester account data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_matrix_less_missing = response_matrix.loc[response_matrix['expert_match?']!='n/a']\n",
    "total_captured = response_matrix_less_missing.groupby(['iteration','n'])['counts'].sum().reset_index(name='totals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge tables to be able to do calculations and obtain total attempts, then calculate accuracy (True Positive/Total) and inaccuracy (Incorrect Response/Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmpresults_df = response_matrix.merge(total_captured,on=(['iteration','n']))\n",
    "tmpresults_df['ratios'] = tmpresults_df['counts']/tmpresults_df['totals']\n",
    "#print(tmpresults_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since accuracy only counts true positives and total attempts, keep only the true positive data and Calculate the mean, max, median, and sem of the accuracy data frame by aggregating over the 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_df = tmpresults_df.loc[tmpresults_df['expert_match?']=='yes']\n",
    "\n",
    "mean_accuracy = accuracy_df.groupby(['n']).ratios.mean().reset_index(name='mean_accuracy')\n",
    "max_accuracy = accuracy_df.groupby(['n']).ratios.max().reset_index(name='max_accuracy')\n",
    "median_accuracy = accuracy_df.groupby(['n']).ratios.median().reset_index(name='median_accuracy')\n",
    "median_q25 = accuracy_df.groupby(['n']).ratios.quantile(0.25).reset_index(name='med_q25')\n",
    "median_q75 = accuracy_df.groupby(['n']).ratios.quantile(0.75).reset_index(name='med_q75')\n",
    "mean_error = accuracy_df.groupby(['n']).ratios.sem().reset_index(name='std_error')\n",
    "mean_dev = accuracy_df.groupby(['n']).ratios.std().reset_index(name='std_dev')\n",
    "    \n",
    "stats_result = mean_accuracy.merge(max_accuracy.merge(median_accuracy.merge(mean_error.merge(mean_dev.merge(median_q25.merge(median_q75, on=(['n']), how='left'), on=(['n']), how='left'), on=(['n']), how='left'), on=(['n']), how='left'), on=(['n']), how='left'), on=(['n']), how='left')\n",
    "print(stats_result.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"plot of mean and standard deviation\")\n",
    "plot_it = stats_result[['mean_accuracy','n','std_dev']].copy()\n",
    "mplot.figure()\n",
    "mplot.xlabel(\"number of workers (n)\")\n",
    "mplot.ylabel(\"Accuracy\")\n",
    "mplot.ylim(0.55, 0.8)\n",
    "mplot.errorbar(x='n',y='mean_accuracy', yerr='std_dev', data=plot_it, linestyle='-', marker='o')\n",
    "mplot.title(\"Number of workers vs accuracy\")\n",
    "mplot.show()\n",
    "#mplot.savefig(exppath+'mean_accuracy_sd_dev.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"plot of median and q25 and q75 quantiles\")\n",
    "plot_it = stats_result[['median_accuracy','n','med_q25','med_q75']].copy()\n",
    "qt_err=numpy.array([stats_result['median_accuracy']-stats_result['med_q25'],stats_result['med_q75']-stats_result['median_accuracy']])\n",
    "mplot.figure()\n",
    "mplot.xlabel(\"number of workers (n)\")\n",
    "mplot.ylabel(\"Accuracy\")\n",
    "mplot.ylim(0.55, 0.80)\n",
    "mplot.errorbar(x='n',y='median_accuracy', yerr=qt_err, data=plot_it, linestyle='-', marker='o')\n",
    "mplot.title(\"Number of workers vs accuracy\")\n",
    "mplot.show()\n",
    "#mplot.savefig(exppath+'median_accuracy_quantiles.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly subsample the user annotations to determine how voting threshold (k) relates to agreement with the quality-checked results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "k=1\n",
    "majority_response_k=[]\n",
    "\n",
    "k=1\n",
    "while k<16:\n",
    "    i=0\n",
    "    while i<10:\n",
    "        for each_cpmid in test_set_cpmids:\n",
    "            result_dict = {}\n",
    "            result_dict['iteration'] = i\n",
    "            result_dict['cpmid'] = each_cpmid\n",
    "            result_dict['k'] = k\n",
    "            tmp_df = total_ref_set.loc[total_ref_set['cpmid']==each_cpmid]\n",
    "            user_set = set(tmp_df['user_id'].tolist())\n",
    "            if k<=len(user_set):\n",
    "                user_sample = random.sample(user_set, k)\n",
    "                ref_sample = tmp_df.loc[tmp_df['user_id'].isin(user_sample)]\n",
    "                if k == 1:\n",
    "                    result_dict['response'] = ref_sample.iloc[0]['evtype']\n",
    "                    result_dict['majority?'] = 'yes'\n",
    "                    result_dict['tie?'] = 'no'\n",
    "                    result_dict['result_selection'] = 'single_vote'\n",
    "                else:\n",
    "                    ## Groupby response and get biggest kvalue    \n",
    "                    ref_sample_size = ref_sample.groupby(['cpmid','evtype']).size().reset_index(name='counts')\n",
    "                    ref_sample_size.sort_values('counts', ascending=False, inplace=True)\n",
    "                    first_most = ref_sample_size.iloc[0]['counts']\n",
    "                    try:\n",
    "                        sec_most = ref_sample_size.iloc[1]['counts'] ##if this works, results are not unanimous\n",
    "                        result_dict['majority?'] = 'n/a'\n",
    "                        result_dict['response'] = 'not_enough'\n",
    "                        result_dict['tie?'] = 'n/a'\n",
    "                        result_dict['result_selection'] = 'n/a' \n",
    "                    except: ## Scenario: results are unanimous\n",
    "                        result_dict['majority?'] = 'unanimous'\n",
    "                        result_dict['response'] = ref_sample_size.iloc[0]['evtype']\n",
    "                        result_dict['tie?'] = 'no'\n",
    "                        result_dict['result_selection'] = 'majority'\n",
    "            else:\n",
    "                result_dict['majority?'] = 'n/a'\n",
    "                result_dict['response'] = 'not_enough'\n",
    "                result_dict['tie?'] = 'n/a'\n",
    "                result_dict['result_selection'] = 'n/a' \n",
    "            ## check how well user response matched expert response\n",
    "            tmp_edf = esample_df.loc[esample_df['cpmid']==each_cpmid]           \n",
    "            test_response = result_dict.get('response')\n",
    "            expert_response = tmp_edf['conclusion'].iloc[0]\n",
    "            if test_response=='not_enough':\n",
    "                result_dict['expert_match?'] = 'n/a'\n",
    "            elif test_response == expert_response:\n",
    "                result_dict['expert_match?'] = 'yes'\n",
    "            else:\n",
    "                result_dict['expert_match?'] = 'no'\n",
    "            majority_response_k.append(result_dict) \n",
    "        i=i+1\n",
    "    k=k+1\n",
    "   \n",
    "\n",
    "majority_df_k = pandas.DataFrame(majority_response_k)\n",
    "\n",
    "print(majority_df_k.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#majority_df_k.to_csv(exppath+'raw_random_k_accuracy_results.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the resulting data, grouping by iteration and threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of expert_match (true positive) vs total number of responses (true positive plus incorrect response) \n",
    "This gives total number of answers that matched vs didn't match the QC'd result for the whole QC cpmid set at every level of n, for each repetition and iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_matrix_less_missing_k = majority_df_k.loc[majority_df_k['expert_match?']!='n/a']\n",
    "response_matrix_k = response_matrix_less_missing_k.groupby(['iteration','k','expert_match?']).size().reset_index(name='counts')\n",
    "print(response_matrix_k.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the n/a responses (this is usually when n exceeds sample due to removal of tester account data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_captured_k = response_matrix_less_missing_k.groupby(['k','iteration']).size().reset_index(name='totals')\n",
    "print(total_captured_k.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge tables to be able to do calculations and obtain total attempts, then calculate accuracy (True Positive/Total) and inaccuracy (Incorrect Response/Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(response_matrix_less_missing.head(n=2))\n",
    "tmpresults_df_k = response_matrix_k.merge(total_captured_k,on=(['k','iteration']), how=\"inner\")\n",
    "tmpresults_df_k['ratios'] = tmpresults_df_k['counts']/tmpresults_df_k['totals']\n",
    "tmpresults_df_k['dropped'] = 116-tmpresults_df_k['totals'].astype(int)\n",
    "print(tmpresults_df_k.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since accuracy only counts true positives and total attempts, keep only the true positive data and Calculate the mean, max, median, and sem of the accuracy data frame by aggregating over the 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaccuracy_df = tmpresults_df_k.loc[tmpresults_df_k['expert_match?']=='yes']\n",
    "print(kaccuracy_df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmean_accuracy = kaccuracy_df.groupby(['k']).ratios.mean().reset_index(name='mean_accuracy')\n",
    "kmax_accuracy = kaccuracy_df.groupby(['k']).ratios.max().reset_index(name='max_accuracy')\n",
    "kmedian_accuracy = kaccuracy_df.groupby(['k']).ratios.median().reset_index(name='median_accuracy')\n",
    "kmedian_q25 = kaccuracy_df.groupby(['k']).ratios.quantile(0.25).reset_index(name='med_q25')\n",
    "kmedian_q75 = kaccuracy_df.groupby(['k']).ratios.quantile(0.75).reset_index(name='med_q75')\n",
    "kmean_error = kaccuracy_df.groupby(['k']).ratios.sem().reset_index(name='std_error')\n",
    "kmean_dev = kaccuracy_df.groupby(['k']).ratios.std().reset_index(name='std_dev')\n",
    "\n",
    "ksample_dev = kaccuracy_df.groupby(['k']).totals.std().reset_index(name='samp_dev')\n",
    "kmean_totals = kaccuracy_df.groupby(['k']).totals.mean().reset_index(name='mean_samples')\n",
    "    \n",
    "kstats_result = kmean_accuracy.merge(kmax_accuracy.merge(kmedian_accuracy.merge(kmean_error.merge(kmean_dev.merge(kmedian_q25.merge(kmedian_q75, on=(['k']), how='left'), on=(['k']), how='left'), on=(['k']), how='left'), on=(['k']), how='left'), on=(['k']), how='left'), on=(['k']), how='left')\n",
    "kstats_result2= kmean_totals.merge(ksample_dev, on=('k'),how='left')\n",
    "print(stats_result.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_it = kstats_result[['mean_accuracy','k','std_dev']].copy()\n",
    "mplot.figure()\n",
    "mplot.xlabel(\"k\")\n",
    "mplot.ylabel(\"Accuracy\")\n",
    "mplot.ylim(0,1)\n",
    "mplot.errorbar(x='k',y='mean_accuracy', yerr='std_dev', data=plot_it, linestyle='-', marker='o')\n",
    "mplot.title(\"Voter threshold vs accuracy\")\n",
    "mplot.show()\n",
    "#mplot.savefig(exppath+'votingthresh_accuracy.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = kstats_result[['mean_accuracy','k','std_dev']].copy()\n",
    "data2 = kstats_result2[['mean_samples','k','samp_dev']].copy()\n",
    "\n",
    "fig, ax1 = mplot.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Accuracy', color=color)\n",
    "ax1.errorbar(x=data1['k'], y=data1['mean_accuracy'],yerr=data1['std_dev'], color=color, linestyle='-', marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax1.set_ylim(0, 1)\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylim(0, 120)\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('samples counted', color=color)  # we already handled the x-label with ax1\n",
    "ax2.errorbar(x=data2['k'], y=data2['mean_samples'],yerr=data2['samp_dev'], color=color, linestyle='-', marker='o')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "mplot.show()\n",
    "#mplot.savefig(exppath+'votingthresh_accuracy_docs.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
