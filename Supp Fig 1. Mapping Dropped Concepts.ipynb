{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Locating Pubtator-dropped Concepts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The positional information of the concepts were not stored when the concepts were imported into the database, instead they are saved in the the pubtator xml files which are stored in the database (but may be subject to updating). As a result, concepts which are stored in the database may be dropped in the pubtator file resulting in difficulties in locating the positional information for that concept.  This script uses the nltk tokenizer to manually obtain the positional data of the concepts which were originally identified by Pubtator, stored in the database, but dropped after the pubtator files were updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import m2c_rel_basic\n",
    "from pandas import read_csv\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import table of pmids, titles, and abstracts derived from stored xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstracts (pubtext):\n",
    "    pubtext['length'] = pubtext['text'].str.len()\n",
    "    pmidslist = pubtext['pmid'].unique().tolist()\n",
    "    abslist = []   \n",
    "    for eachpmid in pmidslist:\n",
    "        absdict = {}\n",
    "        tmpdf = pubtext.loc[pubtext['pmid']==eachpmid]\n",
    "        abstitle = tmpdf['text'].loc[tmpdf['kind']=='t'].iloc[0]\n",
    "        absabs = tmpdf['text'].loc[tmpdf['kind']=='a'].iloc[0]\n",
    "        absdict['pmid']=eachpmid\n",
    "        absdict['title']=abstitle\n",
    "        absdict['abstract']=absabs\n",
    "        absdict['title_end']=len(abstitle)\n",
    "        abslist.append(absdict)\n",
    "    pubtextdf = pandas.DataFrame(abslist)\n",
    "    return(pubtextdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Import the concept annotations that were dropped after a pubtator update. This is exported from the Investigating Concept Annotations notebook/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped_anns_src = 'dropped_by_pubtator.txt'\n",
    "savepath = 'data/'\n",
    "exppath = 'exports/'\n",
    "dropped_anns = read_csv(savepath+dropped_anns_src, delimiter='\\t', header=0)\n",
    "dropped_anns.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "drp_pmidlist = dropped_anns['pmid'].unique().tolist()\n",
    "#drp_pmidlist = [27103203, 27389247, 26419375]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Import the relevant abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubsource = '2017.11.22 pubmed abstracts from db export.txt'   \n",
    "pubtext = read_csv(savepath+pubsource, delimiter='\\t', header=0)\n",
    "pubtextdf = get_abstracts(pubtext)\n",
    "#print(pubtextdf.head(n=2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Mark2Cure, users are currently NOT able to highlight parts of words, hence user annotations should be padded with a space at the beginning and end of the annotation to avoid having the annotation confused with similar text embedded within other words in the abstract.  For example if a user marked 'galactose', this should be mapped to 'galactose' NOT 'galactosemic'.  Note that there are other cases where the annotation may begin or end with a character other than space, and this script will account for a limited number of the most likely cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## pad with spaces and escape characters to avoid breaking up embedded text (eg- 'galactose' from 'galactosemic') \n",
    "a=(\" \",\"\",\"-\",\",\") ## a concept may be the first word in a sentence, so pad with \"\" as well\n",
    "b=(\" \",\"-\",\",\") ## a concept may be the last word in a sentence, but pad with \".\" doesn't work\n",
    "escapeset = pandas.DataFrame([(x,y) for x in a for y in a])\n",
    "#print(escapeset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The search should be prioritized so that it's not unnecessarily repeated. The annotation engulfed in spaces is the most likely scenario, hence it will be the first pass. Only annotations failing to be found in the first pass will need to searched in the other passes. Once found, the script will insert a breakpoint whenever the annotation is found, then the entire text is split by the breakpoints in order to get the positional information on that annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pmid identifier type  no_of_mentions_db in_db no_of_mentions_pub  \\\n",
      "0  501285    C039522    c                1.0   yes                 no   \n",
      "1  501285    D003920    d                3.0   yes                 no   \n",
      "\n",
      "  in_pubtator                                             text  \n",
      "0          no                              glycosylated lysine  \n",
      "1          no  Cataractous lenses of diabetic and galactosemic  \n"
     ]
    }
   ],
   "source": [
    "print(dropped_anns.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4195\n",
      "1701\n",
      "   appear_no               cptext  endset                  exact identifier  \\\n",
      "0          0  glycosylated lysine     365   glycosylated lysine     C039522   \n",
      "1          2  glycosylated lysine    1932   glycosylated lysine     C039522   \n",
      "\n",
      "   length  offset    pmid type  \n",
      "0      19     346  501285    c  \n",
      "1      19    1548  501285    c  \n"
     ]
    }
   ],
   "source": [
    "foundlist = []\n",
    "\n",
    "for eachdrppmid in drp_pmidlist:\n",
    "    anns_to_lookup = dropped_anns.loc[dropped_anns['pmid']==eachdrppmid].reset_index(drop=True)\n",
    "    pub_to_inspect = pubtextdf.loc[pubtextdf['pmid']==eachdrppmid]\n",
    "    n_of_anns_to_lookup = len(anns_to_lookup)\n",
    "    i=0\n",
    "    ## first, check if merge character is in the text, if so, replace it with something else\n",
    "    tmptitle = pub_to_inspect['title'].str.replace('~','/').iloc[0].lower() #get rid of case issues\n",
    "    tmpabstract = pub_to_inspect['abstract'].str.replace('~','/').iloc[0].lower() #get rid of case issues\n",
    "    ## Merge the title and the abstract\n",
    "    tmptotal = tmptitle+'~'+tmpabstract\n",
    "    ## Check if the split character is in the text, if so, replace it with something else\n",
    "    tmpall = tmptotal.lower().replace('|','/')\n",
    "    while i < n_of_anns_to_lookup:    \n",
    "        annfound = 'no'\n",
    "        text_to_inspect = anns_to_lookup['text'].iloc[i].lower()  \n",
    "        annlength=len(text_to_inspect)\n",
    "        escape_df = escapeset.copy()\n",
    "        ## Create the list of annotations (with padding spaces and escape characters to look for.)\n",
    "        escape_df['text_to_match']=escape_df[0].astype(str)+text_to_inspect+escape_df[1].astype(str)\n",
    "        tmp_chk_list = escape_df['text_to_match'].tolist()\n",
    "        ## Search using majority of the cases (combination of \" \"text\" \")\n",
    "        escape_count = 0\n",
    "        ## Check for the first annotation in the list(padded with just spaces), and replace it with the delimiter\n",
    "        text_to_split = tmpall.replace(tmp_chk_list[escape_count],'|')        \n",
    "        splitted_txt = text_to_split.split('|')\n",
    "        n_of_partitions = len(splitted_txt)\n",
    "        ## split the text by the delimiter and get the positional data based on the length of the parts\n",
    "        if n_of_partitions >=2:\n",
    "            k=0\n",
    "            placeholder=0\n",
    "            while k<n_of_partitions:\n",
    "                anndict = {}\n",
    "                anndict['exact'] = tmp_chk_list[escape_count]\n",
    "                anndict['pmid']=eachdrppmid\n",
    "                anndict['cptext']=text_to_inspect\n",
    "                anndict['identifier']= anns_to_lookup['identifier'].iloc[i]\n",
    "                anndict['type']= anns_to_lookup['type'].iloc[i]\n",
    "                anndict['length'] = annlength                \n",
    "                offset = len(splitted_txt[k]) + placeholder\n",
    "                endset = offset + annlength + placeholder\n",
    "                anndict['appear_no'] = k\n",
    "                anndict['offset'] = offset\n",
    "                anndict['endset'] = endset\n",
    "                foundlist.append(anndict)     \n",
    "                placeholder = endset\n",
    "                k=k+2   \n",
    "        else:\n",
    "            ## check for other cases\n",
    "            escape_count = 1\n",
    "            while escape_count < len(escape_df):\n",
    "                text_to_split = tmpall.replace(tmp_chk_list[escape_count],'|')        \n",
    "                splitted_txt = text_to_split.split('|')\n",
    "                n_of_partitions = len(splitted_txt)\n",
    "                if n_of_partitions >=2:\n",
    "                    k=0\n",
    "                    placeholder=0\n",
    "                    while k<n_of_partitions:\n",
    "                        anndict = {}\n",
    "                        anndict['exact'] = tmp_chk_list[escape_count]\n",
    "                        anndict['pmid']=eachdrppmid\n",
    "                        anndict['cptext']=text_to_inspect\n",
    "                        anndict['identifier']= anns_to_lookup['identifier'].iloc[i]\n",
    "                        anndict['type']= anns_to_lookup['type'].iloc[i]\n",
    "                        anndict['length'] = annlength                \n",
    "                        offset = len(splitted_txt[k]) + placeholder\n",
    "                        endset = offset + annlength + placeholder\n",
    "                        anndict['appear_no'] = k\n",
    "                        anndict['offset'] = offset\n",
    "                        anndict['endset'] = endset\n",
    "                        foundlist.append(anndict)     \n",
    "                        placeholder = endset\n",
    "                        k=k+2 \n",
    "                else: \n",
    "                    ## save the failed attempt at matching\n",
    "                    k=0\n",
    "                    placeholder=0\n",
    "                    anndict = {}\n",
    "                    anndict['exact'] = tmp_chk_list[escape_count]\n",
    "                    anndict['pmid']=eachdrppmid\n",
    "                    anndict['cptext']=text_to_inspect\n",
    "                    anndict['identifier']= anns_to_lookup['identifier'].iloc[i]\n",
    "                    anndict['type']= anns_to_lookup['type'].iloc[i]\n",
    "                    anndict['length'] = annlength                \n",
    "                    offset = -1\n",
    "                    endset = -1\n",
    "                    anndict['appear_no'] = -1\n",
    "                    anndict['offset'] = offset\n",
    "                    anndict['endset'] = endset\n",
    "                    foundlist.append(anndict)     \n",
    "                    placeholder = endset\n",
    "                    k=k+2 \n",
    "                escape_count = escape_count+1\n",
    "        i=i+1\n",
    "\n",
    "founddf = pandas.DataFrame(foundlist)\n",
    "\n",
    "dropped_anns_found = founddf.loc[founddf['offset']!=-1]\n",
    "dropped_anns_missing = founddf.loc[founddf['offset']==-1]\n",
    "\n",
    "print(len(dropped_anns_found))\n",
    "print(len(dropped_anns_missing))\n",
    "print(founddf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store the results for concept distance analysis\n",
    "founddf.to_csv(savepath+'dropped_anns_offsets.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use the nltk tokenizers to get the average number of characters, words, and sentences in an abstract.  Use these values to calculate the average number of characters per word, and characters per sentence. This will make it easier to bin concept distances for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pmids with pubtator files in db:  4241\n",
      "number of relevant pmids:  234\n"
     ]
    }
   ],
   "source": [
    "## Limit the tokenization of abstracts to just the abstracts where annotations were completed\n",
    "all_completed_anns = read_csv(savepath+'all_completed_anns.txt', delimiter='\\t', header=0)\n",
    "all_completed_anns.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "relevant_pubtextdf = pubtextdf.loc[pubtextdf['pmid'].isin(set(all_completed_anns['pmid'].tolist()))].copy()\n",
    "print('number of pmids with pubtator files in db: ',len(pubtextdf))\n",
    "print('number of relevant pmids: ',len(relevant_pubtextdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ginger\\Anaconda3\\envs\\py3bioc\\lib\\site-packages\\pandas\\core\\indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              abstract      pmid  \\\n",
      "809  The conserved oligomeric Golgi ( COG ) complex...  23430875   \n",
      "810  Complex carbohydrates are macromolecules biosy...  18269265   \n",
      "\n",
      "                                                 title  title_end  \\\n",
      "809   COG5-CDG with a Mild Neurohepatic Presentation .         48   \n",
      "810  Ion mobility mass spectrometry analysis of hum...         63   \n",
      "\n",
      "     abstract_length  char_count  \\\n",
      "809              853         901   \n",
      "810             1360        1423   \n",
      "\n",
      "                                                  text  sentence_count  \\\n",
      "809  COG5-CDG with a Mild Neurohepatic Presentation...               7   \n",
      "810  Ion mobility mass spectrometry analysis of hum...               6   \n",
      "\n",
      "     word_count  \n",
      "809         155  \n",
      "810         206  \n"
     ]
    }
   ],
   "source": [
    "relevant_pubtextdf['abstract_length']=relevant_pubtextdf['abstract'].str.len()\n",
    "relevant_pubtextdf['char_count'] = relevant_pubtextdf['title_end']+relevant_pubtextdf['abstract_length']\n",
    "relevant_pubtextdf['text'] = relevant_pubtextdf['title']+\"\\n\"+relevant_pubtextdf['abstract']\n",
    "relevant_pubtextdf['sentence_count'] = 0\n",
    "relevant_pubtextdf['word_count'] = 0\n",
    "\n",
    "i=0\n",
    "while i < len(relevant_pubtextdf):\n",
    "    relevant_pubtextdf['sentence_count'].iloc[i] = len(tokenize.sent_tokenize(relevant_pubtextdf['text'].iloc[i],language='english'))\n",
    "    relevant_pubtextdf['word_count'].iloc[i] = len(tokenize.word_tokenize(relevant_pubtextdf['text'].iloc[i],language='english'))\n",
    "    i=i+1 \n",
    "\n",
    "print(relevant_pubtextdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pmid  sentence_count  word_count  title_end  abstract_length  \\\n",
      "809  23430875               7         155         48              853   \n",
      "810  18269265               6         206         63             1360   \n",
      "\n",
      "     char_count  char_p_sent  char_p_word  \n",
      "809         901   128.714286     5.812903  \n",
      "810        1423   237.166667     6.907767  \n"
     ]
    }
   ],
   "source": [
    "tokenized_pmids = relevant_pubtextdf[['pmid','sentence_count','word_count','title_end','abstract_length','char_count']].copy()\n",
    "tokenized_pmids['char_p_sent'] = tokenized_pmids['char_count'].div(tokenized_pmids['sentence_count'])\n",
    "tokenized_pmids['char_p_word'] = tokenized_pmids['char_count'].div(tokenized_pmids['word_count'])\n",
    "print(tokenized_pmids.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## export the data from the tokenized pmids\n",
    "tokenized_pmids.to_csv(exppath+'tokenized_pmids.txt',sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
