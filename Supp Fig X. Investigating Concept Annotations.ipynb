{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Concept Annotation Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook compares the concept annotations that users marked as broken/incorrect with concept annotations that are missing from updated Pubtator files (ie- were dropped by Pubtator).  This book focuses only on concept annotations WITH an associated identifier, since there are issues mapping concepts without identifiers in the Mark2Cure database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as mplot\n",
    "import m2c_rel_basic\n",
    "import relationship_dictionaries\n",
    "from pandas import read_csv\n",
    "from nltk.metrics.scores import precision\n",
    "from nltk.metrics.scores import recall\n",
    "from nltk.metrics.scores import f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relationship annotations data for only completed concept pairs. This data should have already been filtered to remove annotations from test accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savepath = 'data/'\n",
    "exppath = 'results/'\n",
    "all_completed_anns = read_csv(exppath+'all_completed_anns.txt', delimiter='\\t', header=0)\n",
    "all_completed_anns.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pubtator concept annotations from the database export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conceptsource = '2017.11.22 RE pubmed concept export.txt'\n",
    "pubsource = '2017.11.22 pubtator export_parsed_pubtator_anns_from_db_all_anns.txt'\n",
    "all_concept_imported = read_csv(savepath+conceptsource, delimiter='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the updated pubtator files stored in the database and save the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing your data...\n",
      "       pmid     identifier        id_type  \\\n",
      "0  19067230  no identifier  no identifier   \n",
      "1  19067230        D018981           MESH   \n",
      "2  19067230        D006527           MESH   \n",
      "\n",
      "                                   text     type length offset  \n",
      "0                 hypertransaminasaemia  Disease     21     19  \n",
      "1  congenital disorder of glycosylation  Disease     36     51  \n",
      "2                        Wilson disease  Disease     14    321  \n"
     ]
    }
   ],
   "source": [
    "pubtator_key = m2c_rel_basic.get_pub_anns(savepath+pubsource)\n",
    "print(pubtator_key.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_hash_dict,redundant_response_dict,abbreviated_rels_dict,abbreviated_rels_dict_4_hash,concept_broken_dict,concept_not_broken_dict = relationship_dictionaries.load_RE_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original Pubtator Annotations with annotations from updated pubtator files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an active project, the NER algorithms used in Pubtator are steadily improving even as the annotations are used in different projects like Mark2Cure.  Hence, annotations pulled at one timepoint may be inferior to annotations pulled at a later date. To inspect the potential contributions of citizen scientists to an actively improving project like pubtator, we inspect the annotations that are thrown out by the users with respect to the annotations that are thrown out by pubtator later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize format of stored pubtator annotations (that were shown to users) and pubtator annotations (from updated pubtator files) for ease of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##filter pubtator table for pmids currently in the database export of imported annotations\n",
    "rel_pmid_list = all_concept_imported['pmid'].unique().tolist()\n",
    "tmp_pub_key = pubtator_key[['pmid','text','identifier','type','offset','length']].copy()\n",
    "tmp_pub_key['pmid'] = tmp_pub_key['pmid'].astype(int)\n",
    "corresponding_pubs = tmp_pub_key.loc[tmp_pub_key['pmid'].isin(rel_pmid_list)].copy()\n",
    "\n",
    "##format data from the database export and those pulled from updated pubtator files to be the same\n",
    "corresponding_pubs['type'] = corresponding_pubs['type'].astype(str).str[0].str.lower()\n",
    "all_concept_imported.rename(columns={'stype':'type', 'concept_id':'identifier'},inplace=True)\n",
    "all_concept_imported['identifier'].replace({'None':'no identifier'}, inplace=True)\n",
    "\n",
    "## Remove species annotations since they should not be in the concept db\n",
    "no_species = corresponding_pubs[corresponding_pubs['type']!='s']\n",
    "\n",
    "## Save these results for downstream concept distance analysis\n",
    "no_species.to_csv(exppath+'concept_anns_from_updated_pub_files.txt',sep='\\t',header=True)\n",
    "all_concept_imported.to_csv(exppath+'concepts_anns_from_db.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate out pubtator annotations that lack identifiers from both sources. These will be treated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations that were unchanged after pubtator update:  7403  dropped:  2551  added:  3668\n"
     ]
    }
   ],
   "source": [
    "## Remove annotations without identifiers\n",
    "no_nones = no_species.loc[no_species['identifier']!='no identifier']\n",
    "no_nones_concepts = all_concept_imported.loc[all_concept_imported['identifier']!='no identifier']\n",
    "\n",
    "## Remove duplicates from multiple mentions from each annotation source\n",
    "no_dups = no_nones.groupby(['pmid','identifier','type']).size().reset_index(name='no_of_mentions_pub')\n",
    "no_dups['in_pubtator']='yes'\n",
    "\n",
    "no_dups_concepts = no_nones_concepts.groupby(['pmid','identifier','type']).size().reset_index(name='no_of_mentions_db')\n",
    "no_dups_concepts['in_db']='yes'\n",
    "\n",
    "## Outer Merge the formatted tables to identify annotations that did not change, were added, or dropped\n",
    "concept_annotations = no_dups_concepts.merge(no_dups, on=(['pmid','identifier','type']), how='outer').fillna('no')\n",
    "concept_annotations['pmid']=concept_annotations['pmid'].astype(int)\n",
    "\n",
    "unchanged_anns = concept_annotations.loc[(concept_annotations['in_db']=='yes') & \n",
    "                                        (concept_annotations['in_pubtator']=='yes')]\n",
    "dropped_anns = concept_annotations.loc[(concept_annotations['in_db']=='yes') & \n",
    "                                        (concept_annotations['in_pubtator']=='no')]\n",
    "added_anns = concept_annotations.loc[(concept_annotations['in_db']=='no') & \n",
    "                                        (concept_annotations['in_pubtator']=='yes')]  \n",
    "\n",
    "print('Annotations that were unchanged after pubtator update: ',len(unchanged_anns),' dropped: ',len(dropped_anns),' added: ',len(added_anns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pmid identifier type no_of_mentions_db in_db no_of_mentions_pub  \\\n",
      "0  501285    C039522    c                 1   yes                 no   \n",
      "2  501285    D003920    d                 3   yes                 no   \n",
      "\n",
      "  in_pubtator  \n",
      "0          no  \n",
      "2          no  \n"
     ]
    }
   ],
   "source": [
    "print(dropped_anns.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551\n",
      "2737\n",
      "     pmid identifier type no_of_mentions_db in_db no_of_mentions_pub  \\\n",
      "0  501285    C039522    c                 1   yes                 no   \n",
      "1  501285    D003920    d                 3   yes                 no   \n",
      "\n",
      "  in_pubtator                                             text  \n",
      "0          no                              glycosylated lysine  \n",
      "1          no  Cataractous lenses of diabetic and galactosemic  \n"
     ]
    }
   ],
   "source": [
    "## Merge back the dropped annotations to get the text that corresponds to the identifier\n",
    "dropped_anns_with_text = dropped_anns.merge(all_concept_imported, on=(['pmid','identifier','type']), how='left')\n",
    "print(len(dropped_anns))\n",
    "print(len(dropped_anns_with_text))\n",
    "print(dropped_anns_with_text.head(n=2))\n",
    "##Store annotations dropped by pubtator for further downstream processing\n",
    "dropped_anns_with_text.to_csv(exppath+'dropped_by_pubtator.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull the annotations that users marked as broken and see if they are ones that were thrown out by pubtator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Use dictionary to flatten user responses to broken vs not broken\n",
    "#### Do precision, recall, f, setting reference 'broken ann' set as missing vs not missing in pubtator\n",
    "#### Do precision, recall, f, setting reference 'broken ann' set as annotations where at least 6 users agree.\n",
    "#### Chart the two out and compare.\n",
    "\n",
    "#### Alternative subset annotations by whether or not they are missing from pubtator\n",
    "#### Then check precision, recall, f, when reference is at least 6 users. Plot on same graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Remove user annotations for concepts without identifiers\n",
    "nonones_annresults = all_completed_anns.loc[(all_completed_anns['refid1']!='None')&(all_completed_anns['refid2']!='None')].copy()\n",
    "\n",
    "#### Save these annotations for downstream analysis on concept distance\n",
    "nonones_annresults.to_csv(exppath+'REanns_on_concepts_with_identifiers_only.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                           evtype       cp1_hash         cp2_hash  \\\n",
      "0      364  gene has no relation to disease  9621534_59330  9621534_D001750   \n",
      "1      364                       c_1_broken  9621534_59330  9621534_D005764   \n",
      "\n",
      "                     cpmid  ann_status     pmid  \n",
      "0  9621534_59330_x_D001750  not_broken  9621534  \n",
      "1  9621534_59330_x_D005764   c1_broken  9621534  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ginger\\Anaconda3\\envs\\py3bioc\\lib\\site-packages\\pandas\\core\\indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "nonones_annresults['ann_status'] = 'TBD'\n",
    "nonones_annresults['ann_status'].loc[nonones_annresults['evtype']=='c_1_broken'] = 'c1_broken'\n",
    "nonones_annresults['ann_status'].loc[nonones_annresults['evtype']=='c_2_broken'] = 'c2_broken'\n",
    "nonones_annresults['ann_status'].loc[(nonones_annresults['evtype']!='c_1_broken')&(nonones_annresults['evtype']!='c_2_broken')]= 'not_broken'\n",
    "nonones_annresults['cp1_hash'] = nonones_annresults['pmid'].astype(str).str.cat(nonones_annresults['refid1'].astype(str),sep='_')\n",
    "nonones_annresults['cp2_hash'] = nonones_annresults['pmid'].astype(str).str.cat(nonones_annresults['refid2'].astype(str),sep='_')\n",
    "\n",
    "df_to_analyze = nonones_annresults[['user_id','evtype','cp1_hash','cp2_hash','cpmid','ann_status','pmid']].copy()\n",
    "print(df_to_analyze.head(n=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### For each cpmid that was completed by at least 15 users\n",
    "#### Pull the pmids from this set, and use it to limit the reference set.\n",
    "completed_pmids = set(df_to_analyze['pmid'].astype(int).tolist())\n",
    "\n",
    "## Set the dropped annotations as the reference set of broken annotations\n",
    "dropped_anns_id = dropped_anns.loc[dropped_anns['pmid'].isin(completed_pmids)].copy()\n",
    "dropped_anns_id['cphash'] = dropped_anns['pmid'].astype(str).str.cat(dropped_anns['identifier'].astype(str),sep='_')\n",
    "\n",
    "dropped_id_ref_set = set(dropped_anns_id['cphash'].tolist())\n",
    "\n",
    "## Set the kept annotations as the reference set of unbroken annotations\n",
    "kept_anns = unchanged_anns.loc[unchanged_anns['pmid'].isin(completed_pmids)].copy()\n",
    "kept_anns['cphash'] = kept_anns['pmid'].astype(str).str.cat(kept_anns['identifier'].astype(str),sep='_')\n",
    "ref_set = set(kept_anns['cphash'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on k = 1\n",
      "now working on k = 2\n",
      "now working on k = 3\n",
      "now working on k = 4\n",
      "now working on k = 5\n",
      "now working on k = 6\n",
      "now working on k = 7\n",
      "now working on k = 8\n",
      "now working on k = 9\n",
      "now working on k = 10\n",
      "now working on k = 11\n",
      "now working on k = 12\n",
      "now working on k = 13\n",
      "now working on k = 14\n",
      "now working on k = 15\n"
     ]
    }
   ],
   "source": [
    "#### Note, there are over 1000 cpmids to be analyzed, so this part may take awhile\n",
    "## Each of the 1000 cpmids will be sampled for k users 10 times and analyzed for p,r,f\n",
    "k=1\n",
    "cpmidlist = set(df_to_analyze['cpmid'].tolist())\n",
    "### use the cpmidlist below for testing purposes\n",
    "#cpmidlist = ('10068747_5443_x_C536008','10068747_5443_x_C536009','9621534_59330_x_D005764')\n",
    "cpmid_result = []\n",
    "iterations = 10\n",
    "cpmid_sample_larger_than_population =[]\n",
    "while k <= 15:\n",
    "    i=0\n",
    "    print('now working on k =',k)\n",
    "    while i< iterations:\n",
    "        trial_check = {'k':k,'iteration':i}\n",
    "        trial_df = pandas.DataFrame(columns=['user_id','evtype','cp1_hash','cp2_hash','cpmid','ann_status','pmid'])\n",
    "        ## randomly sample users from the set of users that did that particular cpmid to generate a data frame of k responses per cpmid\n",
    "        for eachcpmid in cpmidlist: \n",
    "            tmpdf = df_to_analyze.loc[df_to_analyze['cpmid']==eachcpmid].copy()\n",
    "            user_set = set(tmpdf['user_id'].tolist())\n",
    "            try:\n",
    "                user_sample = random.sample(user_set, k)\n",
    "                tmp_sample = tmpdf.loc[tmpdf['user_id'].isin(user_sample)].copy()\n",
    "                trial_df = pandas.concat((trial_df,tmp_sample)).reset_index(drop=True)\n",
    "            except:\n",
    "                cpmid_sample_larger_than_population.append(eachcpmid)\n",
    "        ## Get a count of the responses\n",
    "        response_count = trial_df.groupby(['cp1_hash','cp2_hash','cpmid','ann_status']).size().reset_index(name='res_count')\n",
    "        ## Deal with unanimous responses\n",
    "        unanimous_subset = response_count.loc[response_count['res_count']==k].copy()\n",
    "        ## Toss unanimous broken responses to broken set, unanimous not broken responses to reference set\n",
    "        u_cp1_broken = set(unanimous_subset['cp1_hash'].loc[unanimous_subset['ann_status']=='c1_broken'].tolist())\n",
    "        u_cp2_broken = set(unanimous_subset['cp2_hash'].loc[unanimous_subset['ann_status']=='c2_broken'].tolist())\n",
    "        u_cp1_fine = set(unanimous_subset['cp1_hash'].loc[unanimous_subset['ann_status']=='not_broken'].tolist())\n",
    "        u_cp2_fine = set(unanimous_subset['cp2_hash'].loc[unanimous_subset['ann_status']=='not_broken'].tolist())\n",
    "        ## Deal with the non-unanimous responses\n",
    "        not_unanimous = response_count.loc[response_count['res_count']<k].copy() #response count should never be more than k\n",
    "        not_unanimous.sort_values(['cpmid','res_count'],ascending=[False,False],inplace=True)\n",
    "        ## Deal with the tied-responses\n",
    "        not_unanimous['tied'] = not_unanimous.duplicated(subset=['cpmid','res_count'],keep=False)\n",
    "        ## Keep the top result if it is NOT a tied result\n",
    "        top_results = not_unanimous.drop_duplicates(subset='cpmid', keep='first',inplace=False)\n",
    "        majority_rules = top_results.loc[top_results['tied']==False].copy()\n",
    "        mj_cp1_broken = set(majority_rules['cp1_hash'].loc[majority_rules['ann_status']=='c1_broken'].tolist())\n",
    "        mj_cp2_broken = set(majority_rules['cp2_hash'].loc[majority_rules['ann_status']=='c2_broken'].tolist())\n",
    "        mj_cp1_fine = set(majority_rules['cp1_hash'].loc[majority_rules['ann_status']=='not_broken'].tolist())\n",
    "        mj_cp2_fine = set(majority_rules['cp2_hash'].loc[majority_rules['ann_status']=='not_broken'].tolist())\n",
    "        ## Generate dataframe of annotations in which the top result was tied\n",
    "        tied_top_cpmids = top_results['cpmid'].loc[top_results['tied']==True].tolist()\n",
    "        tied_results = not_unanimous.loc[not_unanimous['cpmid'].isin(tied_top_cpmids)].copy()\n",
    "        ## randomly select one of the tied results to be the correct response\n",
    "        tied_results['rand_value'] = numpy.random.randint(1, 6, tied_results.shape[0])\n",
    "        tied_results.sort_values(['cpmid','rand_value'],ascending=['False','False'], inplace=True)\n",
    "        tied_untied = tied_results.drop_duplicates(subset='cpmid', keep='first',inplace=False)\n",
    "        tut_cp1_broken = set(tied_untied['cp1_hash'].loc[tied_untied['ann_status']=='c1_broken'].tolist())\n",
    "        tut_cp2_broken = set(tied_untied['cp2_hash'].loc[tied_untied['ann_status']=='c2_broken'].tolist())\n",
    "        tut_cp1_fine = set(tied_untied['cp1_hash'].loc[tied_untied['ann_status']=='not_broken'].tolist())\n",
    "        tut_cp2_fine = set(tied_untied['cp2_hash'].loc[tied_untied['ann_status']=='not_broken'].tolist())\n",
    "        ## Create the sets of cphashes to be checked\n",
    "        broken_set = u_cp1_broken.union(u_cp2_broken.union(mj_cp1_broken.union(mj_cp2_broken.union(tut_cp1_broken.union(tut_cp2_broken)))))\n",
    "        test_set = u_cp1_fine.union(u_cp2_fine.union(mj_cp1_fine.union(mj_cp2_fine.union(tut_cp1_fine.union(tut_cp2_fine)))))\n",
    "        ## Format and store the results\n",
    "        broke_p = precision(dropped_id_ref_set,broken_set)\n",
    "        broke_r= recall(dropped_id_ref_set,broken_set)\n",
    "        broke_f= f_measure(dropped_id_ref_set,broken_set)\n",
    "        prcsn = precision(ref_set,test_set)\n",
    "        rcl = recall(ref_set,test_set)\n",
    "        fmeas = f_measure(ref_set,test_set)\n",
    "        trial_check['total_cpmids'] = len(cpmidlist)\n",
    "        trial_check['unanimous'] = len(unanimous_subset)\n",
    "        trial_check['majority_ruled'] = len(majority_rules)\n",
    "        trial_check['tied_top'] = len(tied_untied)\n",
    "        trial_check['broken_user_anns'] = len(broken_set)\n",
    "        trial_check['not_broken_user_anns'] = len(test_set)\n",
    "        trial_check['broken_prcsn'] = broke_p\n",
    "        trial_check['broken_rcl'] = broke_r\n",
    "        trial_check['broken_fmeas'] = broke_f\n",
    "        trial_check['precision'] = prcsn\n",
    "        trial_check['recall'] = rcl\n",
    "        trial_check['fmeas'] = fmeas\n",
    "        cpmid_result.append(trial_check)\n",
    "        i=i+1\n",
    "    k=k+1\n",
    "\n",
    "prclf = pandas.DataFrame(cpmid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Export the result to save time on having to run it again\n",
    "#prclf.to_csv(exppath+'identified_broken_anns_vs_dropped_pubtators.txt',sep='\\t',header=True)\n",
    "#print(len(set(cpmid_sample_larger_than_population)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import precision, mean, f-score df if needed. \n",
    "prclf = read_csv(exppath+'identified_broken_anns_vs_dropped_pubtators.txt', delimiter='\\t', header=0)\n",
    "prclf.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   broken_fmeas  broken_prcsn  broken_rcl  broken_user_anns     fmeas  \\\n",
      "0      0.276256      0.320106    0.242972               378  0.490855   \n",
      "1      0.294785      0.338542    0.261044               384  0.505325   \n",
      "\n",
      "   iteration  k  majority_ruled  not_broken_user_anns  precision    recall  \\\n",
      "0          0  1               0                   659   0.631259  0.401544   \n",
      "1          1  1               0                   654   0.652905  0.412162   \n",
      "\n",
      "   tied_top  total_cpmids  unanimous  \n",
      "0         0           975        968  \n",
      "1         0           975        965  \n",
      "     k    mean_p    mean_r    mean_f     sem_f     sem_p     sem_r\n",
      "0    1  0.646167  0.416506  0.506489  0.002469  0.002619  0.002623\n",
      "1    2  0.642555  0.400483  0.493396  0.002078  0.001482  0.002461\n",
      "2    3  0.628327  0.385907  0.478105  0.003464  0.002582  0.003662\n",
      "3    4  0.630229  0.380695  0.474654  0.001924  0.002102  0.001861\n",
      "4    5  0.620229  0.365444  0.459875  0.002693  0.002550  0.002750\n",
      "5    6  0.620617  0.366120  0.460501  0.003186  0.002246  0.003393\n",
      "6    7  0.616384  0.363996  0.457677  0.001673  0.001262  0.001949\n",
      "7    8  0.615885  0.358398  0.453077  0.001906  0.001690  0.002248\n",
      "8    9  0.614687  0.358977  0.453230  0.001862  0.000804  0.002117\n",
      "9   10  0.613737  0.355405  0.450130  0.001936  0.001573  0.001935\n",
      "10  11  0.614344  0.354247  0.449361  0.000977  0.001020  0.001160\n",
      "11  12  0.609789  0.350097  0.444810  0.000834  0.001075  0.000776\n",
      "12  13  0.610425  0.350290  0.445135  0.001074  0.001118  0.001032\n",
      "13  14  0.612357  0.341409  0.438396  0.001039  0.001221  0.000911\n",
      "14  15  0.620805  0.292664  0.397796  0.000504  0.000873  0.000375\n"
     ]
    }
   ],
   "source": [
    "#Calculate mean precision, recall, and f-scores\n",
    "print(prclf.head(n=2))\n",
    "mean_fmeas = prclf.groupby('k').fmeas.mean().reset_index(name='mean_f')\n",
    "mean_prcsn = prclf.groupby('k').precision.mean().reset_index(name='mean_p')\n",
    "mean_recall = prclf.groupby('k').recall.mean().reset_index(name='mean_r')\n",
    "sem_fmeas = prclf.groupby('k').fmeas.sem().reset_index(name='sem_f')\n",
    "sem_prcsn = prclf.groupby('k').precision.sem().reset_index(name='sem_p')\n",
    "sem_recall = prclf.groupby('k').recall.sem().reset_index(name='sem_r')\n",
    "stats_result = mean_prcsn.merge(mean_recall.merge(mean_fmeas.merge(sem_fmeas.merge(sem_prcsn.merge(sem_recall, on='k', how='left'), on='k', how='left'), on='k', how='left'), on='k', how='left'), on='k', how='left')\n",
    "print(stats_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the annotations that are getting rejected by users are not necessarily the same annotations getting thrown out by pubtator, suggesting that there are still interesting differences between what pubtator is tossing out vs what the users are throwing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Visualize the results\n",
    "mplot.plot(stats_result['k'],stats_result['mean_p'], linewidth=2.0, color='magenta',marker='o',label=\"precision\")\n",
    "mplot.plot(stats_result['k'],stats_result['mean_r'], linewidth=2.0, color='purple',marker='o',label=\"recall\")\n",
    "mplot.plot(stats_result['k'],stats_result['mean_f'], linewidth=2.0, color='blue',marker='o',label=\"f\")\n",
    "mplot.ylim(0.1,0.9)\n",
    "mplot.xlabel('k')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
